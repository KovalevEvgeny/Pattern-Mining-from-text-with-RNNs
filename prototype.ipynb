{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_etsy.csv')\n",
    "w = df['description'].values\n",
    "w = ''.join(w)\n",
    "\n",
    "vocab = list(string.ascii_lowercase + string.digits + ' ')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data_long_string = w.lower()\n",
    "for char in list(set(data_long_string)):\n",
    "    if char not in vocab:\n",
    "        data_long_string = data_long_string.replace(char, '')\n",
    "data_list_of_chars = np.array(list(data_long_string))\n",
    "\n",
    "map_char_to_float = {char:i for i, char in enumerate(vocab)}\n",
    "def encode_string(string):\n",
    "    return np.array([map_char_to_float[char] for char in string])\n",
    "\n",
    "data_list_of_ints = encode_string(data_list_of_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SEQ_LENGTH = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (16, 37, 512)             18944     \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (16, 37, 256)             787456    \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (16, 37, 256)             525312    \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (16, 37, 256)             525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (16, 37, 37)              9509      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (16, 37, 37)              0         \n",
      "=================================================================\n",
      "Total params: 1,866,533\n",
      "Trainable params: 1,866,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 512, batch_input_shape=(BATCH_SIZE, SEQ_LENGTH)))\n",
    "for i in range(3):\n",
    "    model.add(LSTM(256, return_sequences=True, stateful=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(vocab_size)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(T, vocab_size):\n",
    "    length = T.shape[0]\n",
    "    batch_chars = length // BATCH_SIZE\n",
    "\n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH):\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))\n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, vocab_size))\n",
    "        for batch_idx in range(0, BATCH_SIZE):\n",
    "            for i in range(0, SEQ_LENGTH):\n",
    "                X[batch_idx, i] = T[batch_chars * batch_idx + start + i]\n",
    "                Y[batch_idx, i, T[batch_chars * batch_idx + start + i + 1]] = 1\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss = 3.6111, acc = 0.03378\n",
      "Batch 2: loss = 3.5991, acc = 0.14865\n",
      "Batch 3: loss = 3.5727, acc = 0.15709\n",
      "Batch 4: loss = 3.5041, acc = 0.17061\n",
      "Batch 5: loss = 3.3195, acc = 0.16723\n",
      "Batch 6: loss = 3.5000, acc = 0.16216\n",
      "Batch 7: loss = 3.2732, acc = 0.15034\n",
      "Batch 8: loss = 3.1081, acc = 0.17736\n",
      "Batch 9: loss = 3.1936, acc = 0.08108\n",
      "Batch 10: loss = 3.2175, acc = 0.07939\n",
      "Batch 11: loss = 3.1686, acc = 0.09122\n",
      "Batch 12: loss = 3.1169, acc = 0.09122\n",
      "Batch 13: loss = 3.0877, acc = 0.08615\n",
      "Batch 14: loss = 3.0841, acc = 0.09966\n",
      "Batch 15: loss = 3.1039, acc = 0.08953\n",
      "Batch 16: loss = 3.0152, acc = 0.10811\n",
      "Batch 17: loss = 3.0072, acc = 0.16047\n",
      "Batch 18: loss = 2.9554, acc = 0.15372\n",
      "Batch 19: loss = 2.9496, acc = 0.17568\n",
      "Batch 20: loss = 2.9928, acc = 0.15034\n",
      "Batch 21: loss = 3.0696, acc = 0.16554\n",
      "Batch 22: loss = 3.0155, acc = 0.15034\n",
      "Batch 23: loss = 3.0171, acc = 0.17230\n",
      "Batch 24: loss = 3.0322, acc = 0.15541\n",
      "Batch 25: loss = 3.0212, acc = 0.15203\n",
      "Batch 26: loss = 3.0640, acc = 0.17061\n",
      "Batch 27: loss = 2.9363, acc = 0.16554\n",
      "Batch 28: loss = 2.9636, acc = 0.16385\n",
      "Batch 29: loss = 2.9765, acc = 0.16216\n",
      "Batch 30: loss = 2.9926, acc = 0.16723\n",
      "Batch 31: loss = 2.9792, acc = 0.16892\n",
      "Batch 32: loss = 3.1111, acc = 0.15878\n",
      "Batch 33: loss = 2.9045, acc = 0.16723\n",
      "Batch 34: loss = 2.9916, acc = 0.18412\n",
      "Batch 35: loss = 2.8818, acc = 0.17399\n",
      "Batch 36: loss = 2.9813, acc = 0.17399\n",
      "Batch 37: loss = 2.9530, acc = 0.17399\n",
      "Batch 38: loss = 2.9696, acc = 0.15878\n",
      "Batch 39: loss = 3.0808, acc = 0.15203\n",
      "Batch 40: loss = 3.0474, acc = 0.15203\n",
      "Batch 41: loss = 2.9497, acc = 0.15878\n",
      "Batch 42: loss = 2.9653, acc = 0.16723\n",
      "Batch 43: loss = 2.9334, acc = 0.16385\n",
      "Batch 44: loss = 3.1057, acc = 0.16723\n",
      "Batch 45: loss = 2.9294, acc = 0.21115\n",
      "Batch 46: loss = 3.0517, acc = 0.18243\n",
      "Batch 47: loss = 3.0195, acc = 0.16216\n",
      "Batch 48: loss = 2.9151, acc = 0.17905\n",
      "Batch 49: loss = 2.9304, acc = 0.17736\n",
      "Batch 50: loss = 2.9491, acc = 0.16892\n",
      "Batch 51: loss = 2.9841, acc = 0.16554\n",
      "Batch 52: loss = 2.9883, acc = 0.17061\n",
      "Batch 53: loss = 2.9411, acc = 0.16554\n",
      "Batch 54: loss = 2.9543, acc = 0.15034\n",
      "Batch 55: loss = 2.9159, acc = 0.17568\n",
      "Batch 56: loss = 3.0324, acc = 0.16047\n",
      "Batch 57: loss = 2.9810, acc = 0.15709\n",
      "Batch 58: loss = 2.9269, acc = 0.19932\n",
      "Batch 59: loss = 3.0270, acc = 0.15203\n",
      "Batch 60: loss = 3.0356, acc = 0.14696\n",
      "Batch 61: loss = 3.0468, acc = 0.13007\n",
      "Batch 62: loss = 2.9736, acc = 0.15372\n",
      "Batch 63: loss = 2.9582, acc = 0.14189\n",
      "Batch 64: loss = 3.0616, acc = 0.15878\n",
      "Batch 65: loss = 2.9992, acc = 0.15203\n",
      "Batch 66: loss = 2.9969, acc = 0.15541\n",
      "Batch 67: loss = 2.9606, acc = 0.14696\n",
      "Batch 68: loss = 3.0108, acc = 0.16216\n",
      "Batch 69: loss = 3.0290, acc = 0.14189\n",
      "Batch 70: loss = 2.9562, acc = 0.15709\n",
      "Batch 71: loss = 3.1083, acc = 0.14527\n",
      "Batch 72: loss = 2.9797, acc = 0.15203\n",
      "Batch 73: loss = 3.0864, acc = 0.14696\n",
      "Batch 74: loss = 3.0434, acc = 0.15034\n",
      "Batch 75: loss = 2.9956, acc = 0.16554\n",
      "Batch 76: loss = 2.9716, acc = 0.16385\n",
      "Batch 77: loss = 3.0147, acc = 0.15203\n",
      "Batch 78: loss = 2.9856, acc = 0.16723\n",
      "Batch 79: loss = 2.9781, acc = 0.15541\n",
      "Batch 80: loss = 2.9660, acc = 0.14696\n",
      "Batch 81: loss = 2.9826, acc = 0.15372\n",
      "Batch 82: loss = 2.9870, acc = 0.16047\n",
      "Batch 83: loss = 2.9214, acc = 0.17230\n",
      "Batch 84: loss = 2.9625, acc = 0.16047\n",
      "Batch 85: loss = 2.9925, acc = 0.14358\n",
      "Batch 86: loss = 3.0201, acc = 0.14865\n",
      "Batch 87: loss = 3.0038, acc = 0.15709\n",
      "Batch 88: loss = 2.9315, acc = 0.16216\n",
      "Batch 89: loss = 3.0309, acc = 0.16723\n",
      "Batch 90: loss = 2.9713, acc = 0.16385\n",
      "Batch 91: loss = 2.9985, acc = 0.16554\n",
      "Batch 92: loss = 3.0003, acc = 0.16892\n",
      "Batch 93: loss = 2.9647, acc = 0.15541\n",
      "Batch 94: loss = 3.0068, acc = 0.14189\n",
      "Batch 95: loss = 2.9810, acc = 0.16723\n",
      "Batch 96: loss = 2.9614, acc = 0.16216\n",
      "Batch 97: loss = 2.9190, acc = 0.17568\n",
      "Batch 98: loss = 2.9979, acc = 0.17905\n",
      "Batch 99: loss = 3.0756, acc = 0.16723\n",
      "Batch 100: loss = 3.0377, acc = 0.16892\n",
      "Batch 101: loss = 3.0088, acc = 0.14020\n",
      "Batch 102: loss = 2.9502, acc = 0.14865\n",
      "Batch 103: loss = 3.0286, acc = 0.16723\n",
      "Batch 104: loss = 3.0646, acc = 0.15372\n",
      "Batch 105: loss = 2.9761, acc = 0.16554\n",
      "Batch 106: loss = 3.0763, acc = 0.14527\n",
      "Batch 107: loss = 2.9744, acc = 0.16047\n",
      "Batch 108: loss = 3.0148, acc = 0.16216\n",
      "Batch 109: loss = 3.0073, acc = 0.15709\n",
      "Batch 110: loss = 2.9976, acc = 0.15541\n",
      "Batch 111: loss = 2.9963, acc = 0.15878\n",
      "Batch 112: loss = 3.0362, acc = 0.14020\n",
      "Batch 113: loss = 2.9982, acc = 0.16554\n",
      "Batch 114: loss = 2.9796, acc = 0.16723\n",
      "Batch 115: loss = 3.0158, acc = 0.16216\n",
      "Batch 116: loss = 3.0561, acc = 0.14696\n",
      "Batch 117: loss = 3.0483, acc = 0.17061\n",
      "Batch 118: loss = 3.0438, acc = 0.14865\n",
      "Batch 119: loss = 3.0154, acc = 0.14527\n",
      "Batch 120: loss = 3.0817, acc = 0.15541\n",
      "Batch 121: loss = 3.0585, acc = 0.14865\n",
      "Batch 122: loss = 3.0719, acc = 0.15541\n",
      "Batch 123: loss = 2.9998, acc = 0.16047\n",
      "Batch 124: loss = 3.0376, acc = 0.14696\n",
      "Batch 125: loss = 3.0012, acc = 0.16723\n",
      "Batch 126: loss = 2.9751, acc = 0.16554\n",
      "Batch 127: loss = 3.0284, acc = 0.15372\n",
      "Batch 128: loss = 3.0541, acc = 0.15203\n",
      "Batch 129: loss = 3.0015, acc = 0.15034\n",
      "Batch 130: loss = 3.0150, acc = 0.15878\n",
      "Batch 131: loss = 2.9702, acc = 0.16216\n",
      "Batch 132: loss = 3.0360, acc = 0.16723\n",
      "Batch 133: loss = 2.9934, acc = 0.17230\n",
      "Batch 134: loss = 3.0193, acc = 0.15034\n",
      "Batch 135: loss = 2.9924, acc = 0.16723\n",
      "Batch 136: loss = 3.0332, acc = 0.15372\n",
      "Batch 137: loss = 3.0174, acc = 0.14696\n",
      "Batch 138: loss = 2.9879, acc = 0.17905\n",
      "Batch 139: loss = 2.9680, acc = 0.16554\n",
      "Batch 140: loss = 3.0248, acc = 0.15541\n",
      "Batch 141: loss = 3.0114, acc = 0.20946\n",
      "Batch 142: loss = 3.0365, acc = 0.15203\n",
      "Batch 143: loss = 3.0920, acc = 0.16385\n",
      "Batch 144: loss = 3.0134, acc = 0.16892\n",
      "Batch 145: loss = 3.0721, acc = 0.16723\n",
      "Batch 146: loss = 3.0613, acc = 0.16892\n",
      "Batch 147: loss = 2.9630, acc = 0.16554\n",
      "Batch 148: loss = 3.0219, acc = 0.14189\n",
      "Batch 149: loss = 2.9894, acc = 0.15372\n",
      "Batch 150: loss = 2.9502, acc = 0.17568\n",
      "Batch 151: loss = 3.0016, acc = 0.15372\n",
      "Batch 152: loss = 3.0267, acc = 0.14189\n",
      "Batch 153: loss = 2.9460, acc = 0.16216\n",
      "Batch 154: loss = 3.0225, acc = 0.14527\n",
      "Batch 155: loss = 2.9652, acc = 0.16385\n",
      "Batch 156: loss = 2.9242, acc = 0.16723\n",
      "Batch 157: loss = 2.9679, acc = 0.15541\n",
      "Batch 158: loss = 2.9641, acc = 0.17061\n",
      "Batch 159: loss = 2.9817, acc = 0.15709\n",
      "Batch 160: loss = 2.9466, acc = 0.15878\n",
      "Batch 161: loss = 3.0157, acc = 0.15034\n",
      "Batch 162: loss = 2.9914, acc = 0.15709\n",
      "Batch 163: loss = 2.9704, acc = 0.16385\n",
      "Batch 164: loss = 2.9259, acc = 0.15034\n",
      "Batch 165: loss = 3.0174, acc = 0.15034\n",
      "Batch 166: loss = 3.0144, acc = 0.16892\n",
      "Batch 167: loss = 2.9195, acc = 0.15878\n",
      "Batch 168: loss = 2.9307, acc = 0.16892\n",
      "Batch 169: loss = 2.9403, acc = 0.15541\n",
      "Batch 170: loss = 2.9083, acc = 0.15541\n",
      "Batch 171: loss = 2.9692, acc = 0.16892\n",
      "Batch 172: loss = 2.9056, acc = 0.18581\n",
      "Batch 173: loss = 2.9021, acc = 0.17399\n",
      "Batch 174: loss = 3.0296, acc = 0.14527\n",
      "Batch 175: loss = 2.9514, acc = 0.15034\n",
      "Batch 176: loss = 2.9589, acc = 0.17905\n",
      "Batch 177: loss = 2.8823, acc = 0.16723\n",
      "Batch 178: loss = 2.8669, acc = 0.19595\n",
      "Batch 179: loss = 2.8831, acc = 0.20101\n",
      "Batch 180: loss = 2.8138, acc = 0.21959\n",
      "Batch 181: loss = 2.8422, acc = 0.19426\n",
      "Batch 182: loss = 2.9119, acc = 0.17905\n",
      "Batch 183: loss = 2.9316, acc = 0.17399\n",
      "Batch 184: loss = 2.8173, acc = 0.19595\n",
      "Batch 185: loss = 2.8635, acc = 0.18581\n",
      "Batch 186: loss = 2.8671, acc = 0.19257\n",
      "Batch 187: loss = 2.8518, acc = 0.20439\n",
      "Batch 188: loss = 2.8736, acc = 0.18919\n",
      "Batch 189: loss = 2.8729, acc = 0.19932\n",
      "Batch 190: loss = 2.8765, acc = 0.19426\n",
      "Batch 191: loss = 2.8181, acc = 0.19088\n",
      "Batch 192: loss = 2.8740, acc = 0.22128\n",
      "Batch 193: loss = 2.9394, acc = 0.17736\n",
      "Batch 194: loss = 2.7869, acc = 0.22297\n",
      "Batch 195: loss = 2.8729, acc = 0.19257\n",
      "Batch 196: loss = 2.8504, acc = 0.18750\n",
      "Batch 197: loss = 2.8442, acc = 0.18243\n",
      "Batch 198: loss = 2.8983, acc = 0.20101\n",
      "Batch 199: loss = 2.8604, acc = 0.18750\n",
      "Batch 200: loss = 2.7696, acc = 0.21622\n",
      "Batch 201: loss = 2.8290, acc = 0.21959\n",
      "Batch 202: loss = 2.8109, acc = 0.20439\n",
      "Batch 203: loss = 2.8042, acc = 0.20101\n",
      "Batch 204: loss = 2.7433, acc = 0.22128\n",
      "Batch 205: loss = 2.8660, acc = 0.19088\n",
      "Batch 206: loss = 2.9474, acc = 0.18581\n",
      "Batch 207: loss = 2.8209, acc = 0.18581\n",
      "Batch 208: loss = 2.8502, acc = 0.18581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 209: loss = 2.7817, acc = 0.20439\n",
      "Batch 210: loss = 2.8051, acc = 0.20270\n",
      "Batch 211: loss = 2.8304, acc = 0.18919\n",
      "Batch 212: loss = 2.8474, acc = 0.17905\n",
      "Batch 213: loss = 2.8383, acc = 0.18750\n",
      "Batch 214: loss = 2.8524, acc = 0.19595\n",
      "Batch 215: loss = 2.8043, acc = 0.19088\n",
      "Batch 216: loss = 2.8164, acc = 0.21115\n",
      "Batch 217: loss = 2.7778, acc = 0.20439\n",
      "Batch 218: loss = 2.8315, acc = 0.19595\n",
      "Batch 219: loss = 2.7641, acc = 0.19257\n",
      "Batch 220: loss = 2.7830, acc = 0.17905\n",
      "Batch 221: loss = 2.7223, acc = 0.17905\n",
      "Batch 222: loss = 2.7162, acc = 0.22973\n",
      "Batch 223: loss = 2.7265, acc = 0.20777\n",
      "Batch 224: loss = 2.7607, acc = 0.20101\n",
      "Batch 225: loss = 2.7648, acc = 0.21791\n",
      "Batch 226: loss = 2.6759, acc = 0.21453\n",
      "Batch 227: loss = 2.7454, acc = 0.22635\n",
      "Batch 228: loss = 2.6514, acc = 0.21284\n",
      "Batch 229: loss = 2.6429, acc = 0.24662\n",
      "Batch 230: loss = 2.5909, acc = 0.25000\n",
      "Batch 231: loss = 2.6492, acc = 0.22466\n",
      "Batch 232: loss = 2.5970, acc = 0.23986\n",
      "Batch 233: loss = 2.5827, acc = 0.23649\n",
      "Batch 234: loss = 2.6614, acc = 0.22973\n",
      "Batch 235: loss = 2.6023, acc = 0.22804\n",
      "Batch 236: loss = 2.5468, acc = 0.25845\n",
      "Batch 237: loss = 2.6109, acc = 0.24662\n",
      "Batch 238: loss = 2.6541, acc = 0.22804\n",
      "Batch 239: loss = 2.6658, acc = 0.24155\n",
      "Batch 240: loss = 2.6289, acc = 0.24831\n",
      "Batch 241: loss = 2.5756, acc = 0.27027\n",
      "Batch 242: loss = 2.7118, acc = 0.23986\n",
      "Batch 243: loss = 2.6163, acc = 0.24831\n",
      "Batch 244: loss = 2.6615, acc = 0.22804\n",
      "Batch 245: loss = 2.6330, acc = 0.24324\n",
      "Batch 246: loss = 2.6189, acc = 0.26520\n",
      "Batch 247: loss = 2.6186, acc = 0.25338\n",
      "Batch 248: loss = 2.6940, acc = 0.22635\n",
      "Batch 249: loss = 2.6441, acc = 0.24831\n",
      "Batch 250: loss = 2.6274, acc = 0.24324\n",
      "Batch 251: loss = 2.6935, acc = 0.22635\n",
      "Batch 252: loss = 2.6348, acc = 0.25338\n",
      "Batch 253: loss = 2.5673, acc = 0.25169\n",
      "Batch 254: loss = 2.5016, acc = 0.26014\n",
      "Batch 255: loss = 2.4996, acc = 0.26858\n",
      "Batch 256: loss = 2.4828, acc = 0.27703\n",
      "Batch 257: loss = 2.5755, acc = 0.27872\n",
      "Batch 258: loss = 2.4966, acc = 0.27027\n",
      "Batch 259: loss = 2.4762, acc = 0.27027\n",
      "Batch 260: loss = 2.5523, acc = 0.24324\n",
      "Batch 261: loss = 2.5525, acc = 0.26182\n",
      "Batch 262: loss = 2.6112, acc = 0.23986\n",
      "Batch 263: loss = 2.4250, acc = 0.27872\n",
      "Batch 264: loss = 2.5420, acc = 0.24324\n",
      "Batch 265: loss = 2.4190, acc = 0.28885\n",
      "Batch 266: loss = 2.5042, acc = 0.26858\n",
      "Batch 267: loss = 2.4566, acc = 0.27365\n",
      "Batch 268: loss = 2.5816, acc = 0.22635\n",
      "Batch 269: loss = 2.6668, acc = 0.20946\n",
      "Batch 270: loss = 2.5101, acc = 0.23818\n",
      "Batch 271: loss = 2.5424, acc = 0.25507\n",
      "Batch 272: loss = 2.5388, acc = 0.25507\n",
      "Batch 273: loss = 2.4804, acc = 0.27027\n",
      "Batch 274: loss = 2.4770, acc = 0.26520\n",
      "Batch 275: loss = 2.4756, acc = 0.27027\n",
      "Batch 276: loss = 2.5153, acc = 0.25000\n",
      "Batch 277: loss = 2.5004, acc = 0.26858\n",
      "Batch 278: loss = 2.5643, acc = 0.26351\n",
      "Batch 279: loss = 2.5088, acc = 0.26520\n",
      "Batch 280: loss = 2.5081, acc = 0.28041\n",
      "Batch 281: loss = 2.4248, acc = 0.29054\n",
      "Batch 282: loss = 2.5722, acc = 0.25845\n",
      "Batch 283: loss = 2.4152, acc = 0.28716\n",
      "Batch 284: loss = 2.3993, acc = 0.27027\n",
      "Batch 285: loss = 2.4745, acc = 0.26182\n",
      "Batch 286: loss = 2.4496, acc = 0.27872\n",
      "Batch 287: loss = 2.4168, acc = 0.29561\n",
      "Batch 288: loss = 2.4496, acc = 0.29561\n",
      "Batch 289: loss = 2.4862, acc = 0.26351\n",
      "Batch 290: loss = 2.3868, acc = 0.29392\n",
      "Batch 291: loss = 2.3478, acc = 0.33277\n",
      "Batch 292: loss = 2.3518, acc = 0.28885\n",
      "Batch 293: loss = 2.3540, acc = 0.31081\n",
      "Batch 294: loss = 2.3839, acc = 0.29223\n",
      "Batch 295: loss = 2.4054, acc = 0.30236\n",
      "Batch 296: loss = 2.4627, acc = 0.28716\n",
      "Batch 297: loss = 2.3914, acc = 0.32939\n",
      "Batch 298: loss = 2.4410, acc = 0.26520\n",
      "Batch 299: loss = 2.4475, acc = 0.28885\n",
      "Batch 300: loss = 2.4499, acc = 0.28547\n",
      "Batch 301: loss = 2.3002, acc = 0.33108\n",
      "Batch 302: loss = 2.3378, acc = 0.28547\n",
      "Batch 303: loss = 2.3275, acc = 0.31419\n",
      "Batch 304: loss = 2.3217, acc = 0.31588\n",
      "Batch 305: loss = 2.4216, acc = 0.29899\n",
      "Batch 306: loss = 2.3690, acc = 0.32264\n",
      "Batch 307: loss = 2.4023, acc = 0.31419\n",
      "Batch 308: loss = 2.2766, acc = 0.31419\n",
      "Batch 309: loss = 2.3040, acc = 0.34459\n",
      "Batch 310: loss = 2.3261, acc = 0.30068\n",
      "Batch 311: loss = 2.3020, acc = 0.32095\n",
      "Batch 312: loss = 2.3609, acc = 0.29899\n",
      "Batch 313: loss = 2.2632, acc = 0.32770\n",
      "Batch 314: loss = 2.2136, acc = 0.33446\n",
      "Batch 315: loss = 2.2885, acc = 0.32095\n",
      "Batch 316: loss = 2.3376, acc = 0.29561\n",
      "Batch 317: loss = 2.3150, acc = 0.32095\n",
      "Batch 318: loss = 2.2926, acc = 0.32264\n",
      "Batch 319: loss = 2.3165, acc = 0.29899\n",
      "Batch 320: loss = 2.4600, acc = 0.27872\n",
      "Batch 321: loss = 2.2747, acc = 0.32601\n",
      "Batch 322: loss = 2.3414, acc = 0.30743\n",
      "Batch 323: loss = 2.2646, acc = 0.32939\n",
      "Batch 324: loss = 2.3830, acc = 0.31757\n",
      "Batch 325: loss = 2.4051, acc = 0.29223\n",
      "Batch 326: loss = 2.2820, acc = 0.32939\n",
      "Batch 327: loss = 2.2241, acc = 0.32770\n",
      "Batch 328: loss = 2.3108, acc = 0.30068\n",
      "Batch 329: loss = 2.2993, acc = 0.34966\n",
      "Batch 330: loss = 2.3266, acc = 0.28547\n",
      "Batch 331: loss = 2.3754, acc = 0.30912\n",
      "Batch 332: loss = 2.2994, acc = 0.30743\n",
      "Batch 333: loss = 2.3560, acc = 0.33446\n",
      "Batch 334: loss = 2.2355, acc = 0.34291\n",
      "Batch 335: loss = 2.2512, acc = 0.32432\n",
      "Batch 336: loss = 2.2453, acc = 0.32601\n",
      "Batch 337: loss = 2.2593, acc = 0.32770\n",
      "Batch 338: loss = 2.2949, acc = 0.34122\n",
      "Batch 339: loss = 2.3181, acc = 0.31419\n",
      "Batch 340: loss = 2.4155, acc = 0.30236\n",
      "Batch 341: loss = 2.1882, acc = 0.36824\n",
      "Batch 342: loss = 2.2746, acc = 0.32939\n",
      "Batch 343: loss = 2.2697, acc = 0.33784\n",
      "Batch 344: loss = 2.2141, acc = 0.33277\n",
      "Batch 345: loss = 2.2407, acc = 0.34797\n",
      "Batch 346: loss = 2.2919, acc = 0.33446\n",
      "Batch 347: loss = 2.2598, acc = 0.34122\n",
      "Batch 348: loss = 2.1558, acc = 0.34797\n",
      "Batch 349: loss = 2.2934, acc = 0.34291\n",
      "Batch 350: loss = 2.2822, acc = 0.32264\n",
      "Batch 351: loss = 2.3230, acc = 0.32264\n",
      "Batch 352: loss = 2.2217, acc = 0.32095\n",
      "Batch 353: loss = 2.1967, acc = 0.35473\n",
      "Batch 354: loss = 2.1978, acc = 0.35980\n",
      "Batch 355: loss = 2.2003, acc = 0.34459\n",
      "Batch 356: loss = 2.2679, acc = 0.34459\n",
      "Batch 357: loss = 2.3242, acc = 0.32264\n",
      "Batch 358: loss = 2.4077, acc = 0.29054\n",
      "Batch 359: loss = 2.1605, acc = 0.36824\n",
      "Batch 360: loss = 2.2382, acc = 0.33108\n",
      "Batch 361: loss = 2.1952, acc = 0.35473\n",
      "Batch 362: loss = 2.2102, acc = 0.35980\n",
      "Batch 363: loss = 2.1932, acc = 0.34628\n",
      "Batch 364: loss = 2.2036, acc = 0.34966\n",
      "Batch 365: loss = 2.1500, acc = 0.37669\n",
      "Batch 366: loss = 2.1579, acc = 0.37669\n",
      "Batch 367: loss = 2.3997, acc = 0.32264\n",
      "Batch 368: loss = 2.2695, acc = 0.33953\n",
      "Batch 369: loss = 2.2921, acc = 0.32264\n",
      "Batch 370: loss = 2.1922, acc = 0.33784\n",
      "Batch 371: loss = 2.1190, acc = 0.36486\n",
      "Batch 372: loss = 2.1895, acc = 0.32264\n",
      "Batch 373: loss = 2.1270, acc = 0.36824\n",
      "Batch 374: loss = 2.2629, acc = 0.33277\n",
      "Batch 375: loss = 2.0938, acc = 0.37162\n",
      "Batch 376: loss = 2.0876, acc = 0.38514\n",
      "Batch 377: loss = 2.2905, acc = 0.29899\n",
      "Batch 378: loss = 2.1951, acc = 0.37162\n",
      "Batch 379: loss = 2.1618, acc = 0.37331\n",
      "Batch 380: loss = 2.2768, acc = 0.35473\n",
      "Batch 381: loss = 2.0845, acc = 0.42230\n",
      "Batch 382: loss = 2.1769, acc = 0.34122\n",
      "Batch 383: loss = 2.1448, acc = 0.37500\n",
      "Batch 384: loss = 2.2743, acc = 0.33446\n",
      "Batch 385: loss = 2.1549, acc = 0.38514\n",
      "Batch 386: loss = 2.2075, acc = 0.36149\n",
      "Batch 387: loss = 2.0723, acc = 0.39527\n",
      "Batch 388: loss = 2.1609, acc = 0.37669\n",
      "Batch 389: loss = 2.1399, acc = 0.32939\n",
      "Batch 390: loss = 2.1208, acc = 0.40372\n",
      "Batch 391: loss = 2.2846, acc = 0.31757\n",
      "Batch 392: loss = 2.2987, acc = 0.31419\n",
      "Batch 393: loss = 1.9767, acc = 0.40034\n",
      "Batch 394: loss = 2.1004, acc = 0.37162\n",
      "Batch 395: loss = 2.2454, acc = 0.34459\n",
      "Batch 396: loss = 1.9145, acc = 0.41892\n",
      "Batch 397: loss = 2.1051, acc = 0.37669\n",
      "Batch 398: loss = 2.0568, acc = 0.39358\n",
      "Batch 399: loss = 2.1938, acc = 0.35642\n",
      "Batch 400: loss = 2.2283, acc = 0.33446\n",
      "Batch 401: loss = 2.0029, acc = 0.41554\n",
      "Batch 402: loss = 2.0600, acc = 0.37669\n",
      "Batch 403: loss = 2.1073, acc = 0.36655\n",
      "Batch 404: loss = 2.1453, acc = 0.38851\n",
      "Batch 405: loss = 2.1638, acc = 0.35473\n",
      "Batch 406: loss = 2.2375, acc = 0.32432\n",
      "Batch 407: loss = 2.1035, acc = 0.39020\n",
      "Batch 408: loss = 1.9028, acc = 0.43919\n",
      "Batch 409: loss = 2.0557, acc = 0.35980\n",
      "Batch 410: loss = 2.1984, acc = 0.37838\n",
      "Batch 411: loss = 2.1015, acc = 0.41385\n",
      "Batch 412: loss = 2.0415, acc = 0.41047\n",
      "Batch 413: loss = 2.0678, acc = 0.40034\n",
      "Batch 414: loss = 1.9595, acc = 0.41216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 415: loss = 2.0958, acc = 0.36149\n",
      "Batch 416: loss = 2.0965, acc = 0.37838\n",
      "Batch 417: loss = 2.1650, acc = 0.37669\n",
      "Batch 418: loss = 1.9984, acc = 0.39189\n",
      "Batch 419: loss = 2.0636, acc = 0.38851\n",
      "Batch 420: loss = 2.1626, acc = 0.35811\n",
      "Batch 421: loss = 2.1699, acc = 0.33784\n",
      "Batch 422: loss = 2.1920, acc = 0.36149\n",
      "Batch 423: loss = 2.2671, acc = 0.32770\n",
      "Batch 424: loss = 2.1353, acc = 0.40034\n",
      "Batch 425: loss = 2.1589, acc = 0.37162\n",
      "Batch 426: loss = 2.1745, acc = 0.33277\n",
      "Batch 427: loss = 2.1476, acc = 0.37331\n",
      "Batch 428: loss = 2.1192, acc = 0.37331\n",
      "Batch 429: loss = 2.1267, acc = 0.36318\n",
      "Batch 430: loss = 2.0577, acc = 0.38345\n",
      "Batch 431: loss = 2.0851, acc = 0.38345\n",
      "Batch 432: loss = 2.0282, acc = 0.41385\n",
      "Batch 433: loss = 2.1364, acc = 0.34797\n",
      "Batch 434: loss = 2.2029, acc = 0.34122\n",
      "Batch 435: loss = 2.0917, acc = 0.39189\n",
      "Batch 436: loss = 2.1687, acc = 0.37838\n",
      "Batch 437: loss = 2.0222, acc = 0.40372\n",
      "Batch 438: loss = 2.0800, acc = 0.39865\n",
      "Batch 439: loss = 2.0960, acc = 0.39358\n",
      "Batch 440: loss = 2.0757, acc = 0.40541\n",
      "Batch 441: loss = 2.0480, acc = 0.36824\n",
      "Batch 442: loss = 1.9886, acc = 0.40034\n",
      "Batch 443: loss = 2.0103, acc = 0.40034\n",
      "Batch 444: loss = 2.0272, acc = 0.39527\n",
      "Batch 445: loss = 2.1035, acc = 0.38514\n",
      "Batch 446: loss = 2.0091, acc = 0.42230\n",
      "Batch 447: loss = 1.9682, acc = 0.39865\n",
      "Batch 448: loss = 2.1659, acc = 0.36318\n",
      "Batch 449: loss = 2.1336, acc = 0.37838\n",
      "Batch 450: loss = 2.0138, acc = 0.42736\n",
      "Batch 451: loss = 1.9671, acc = 0.41385\n",
      "Batch 452: loss = 2.1325, acc = 0.38682\n",
      "Batch 453: loss = 2.0575, acc = 0.40372\n",
      "Batch 454: loss = 2.0238, acc = 0.40541\n",
      "Batch 455: loss = 1.9786, acc = 0.42061\n",
      "Batch 456: loss = 2.0339, acc = 0.38176\n",
      "Batch 457: loss = 2.0429, acc = 0.40203\n",
      "Batch 458: loss = 2.0299, acc = 0.39527\n",
      "Batch 459: loss = 2.0130, acc = 0.42568\n",
      "Batch 460: loss = 2.1563, acc = 0.36318\n",
      "Batch 461: loss = 2.0921, acc = 0.38176\n",
      "Batch 462: loss = 2.1251, acc = 0.38176\n",
      "Batch 463: loss = 2.1165, acc = 0.39696\n",
      "Batch 464: loss = 2.0055, acc = 0.40709\n",
      "Batch 465: loss = 1.9250, acc = 0.43581\n",
      "Batch 466: loss = 1.9888, acc = 0.38851\n",
      "Batch 467: loss = 1.9124, acc = 0.44257\n",
      "Batch 468: loss = 2.1286, acc = 0.36318\n",
      "Batch 469: loss = 2.0880, acc = 0.38007\n",
      "Batch 470: loss = 2.0827, acc = 0.40541\n",
      "Batch 471: loss = 1.9377, acc = 0.43581\n",
      "Batch 472: loss = 1.9920, acc = 0.41892\n",
      "Batch 473: loss = 1.9742, acc = 0.43412\n",
      "Batch 474: loss = 2.0648, acc = 0.37838\n",
      "Batch 475: loss = 2.0999, acc = 0.41216\n",
      "Batch 476: loss = 2.0485, acc = 0.37331\n",
      "Batch 477: loss = 1.9410, acc = 0.39527\n",
      "Batch 478: loss = 2.0030, acc = 0.41385\n",
      "Batch 479: loss = 2.0582, acc = 0.39696\n",
      "Batch 480: loss = 1.9873, acc = 0.39189\n",
      "Batch 481: loss = 2.0458, acc = 0.41216\n",
      "Batch 482: loss = 2.0531, acc = 0.38176\n",
      "Batch 483: loss = 2.0936, acc = 0.38851\n",
      "Batch 484: loss = 2.1044, acc = 0.38514\n",
      "Batch 485: loss = 2.1399, acc = 0.37162\n",
      "Batch 486: loss = 2.0948, acc = 0.37838\n",
      "Batch 487: loss = 2.0102, acc = 0.39865\n",
      "Batch 488: loss = 2.0102, acc = 0.37838\n",
      "Batch 489: loss = 1.9625, acc = 0.41892\n",
      "Batch 490: loss = 1.9732, acc = 0.41554\n",
      "Batch 491: loss = 2.0435, acc = 0.40541\n",
      "Batch 492: loss = 1.9888, acc = 0.41723\n",
      "Batch 493: loss = 1.9867, acc = 0.41892\n",
      "Batch 494: loss = 1.9914, acc = 0.42061\n",
      "Batch 495: loss = 2.0026, acc = 0.38682\n",
      "Batch 496: loss = 2.0692, acc = 0.39358\n",
      "Batch 497: loss = 2.0042, acc = 0.42399\n",
      "Batch 498: loss = 2.1457, acc = 0.36149\n",
      "Batch 499: loss = 2.0863, acc = 0.38682\n",
      "Batch 500: loss = 2.1110, acc = 0.38851\n",
      "Batch 501: loss = 1.9474, acc = 0.43074\n",
      "Batch 502: loss = 1.9511, acc = 0.43581\n",
      "Batch 503: loss = 2.0225, acc = 0.39020\n",
      "Batch 504: loss = 2.1159, acc = 0.38514\n",
      "Batch 505: loss = 2.0694, acc = 0.39358\n",
      "Batch 506: loss = 2.1046, acc = 0.36824\n",
      "Batch 507: loss = 2.0608, acc = 0.40709\n",
      "Batch 508: loss = 2.2132, acc = 0.35642\n",
      "Batch 509: loss = 2.1345, acc = 0.35642\n",
      "Batch 510: loss = 2.0179, acc = 0.43412\n",
      "Batch 511: loss = 1.9095, acc = 0.41047\n",
      "Batch 512: loss = 1.9706, acc = 0.41216\n",
      "Batch 513: loss = 1.8732, acc = 0.45777\n",
      "Batch 514: loss = 2.0341, acc = 0.39189\n",
      "Batch 515: loss = 2.0034, acc = 0.40878\n",
      "Batch 516: loss = 1.9928, acc = 0.43412\n",
      "Batch 517: loss = 2.0763, acc = 0.40203\n",
      "Batch 518: loss = 2.0123, acc = 0.41723\n",
      "Batch 519: loss = 1.9857, acc = 0.40034\n",
      "Batch 520: loss = 1.9475, acc = 0.42061\n",
      "Batch 521: loss = 2.0143, acc = 0.40203\n",
      "Batch 522: loss = 1.9615, acc = 0.41723\n",
      "Batch 523: loss = 1.8817, acc = 0.44257\n",
      "Batch 524: loss = 2.0483, acc = 0.40203\n",
      "Batch 525: loss = 2.2331, acc = 0.34966\n",
      "Batch 526: loss = 2.1025, acc = 0.38007\n",
      "Batch 527: loss = 1.9940, acc = 0.41554\n",
      "Batch 528: loss = 2.0450, acc = 0.40203\n",
      "Batch 529: loss = 1.8696, acc = 0.47128\n",
      "Batch 530: loss = 1.9921, acc = 0.39527\n",
      "Batch 531: loss = 1.9966, acc = 0.42230\n",
      "Batch 532: loss = 1.9380, acc = 0.45439\n",
      "Batch 533: loss = 1.9771, acc = 0.42568\n",
      "Batch 534: loss = 1.7413, acc = 0.48480\n",
      "Batch 535: loss = 1.9855, acc = 0.40878\n",
      "Batch 536: loss = 2.0728, acc = 0.39189\n",
      "Batch 537: loss = 1.9859, acc = 0.42399\n",
      "Batch 538: loss = 1.8720, acc = 0.46453\n",
      "Batch 539: loss = 2.0317, acc = 0.41723\n",
      "Batch 540: loss = 1.9037, acc = 0.44932\n",
      "Batch 541: loss = 1.9804, acc = 0.43074\n",
      "Batch 542: loss = 1.8869, acc = 0.42061\n",
      "Batch 543: loss = 1.8339, acc = 0.47128\n",
      "Batch 544: loss = 1.9264, acc = 0.42230\n",
      "Batch 545: loss = 1.9682, acc = 0.42399\n",
      "Batch 546: loss = 1.9027, acc = 0.43750\n",
      "Batch 547: loss = 1.8091, acc = 0.44426\n",
      "Batch 548: loss = 1.8947, acc = 0.45439\n",
      "Batch 549: loss = 1.9808, acc = 0.41554\n",
      "Batch 550: loss = 1.7838, acc = 0.48818\n",
      "Batch 551: loss = 1.8665, acc = 0.43581\n",
      "Batch 552: loss = 1.9237, acc = 0.44932\n",
      "Batch 553: loss = 2.1203, acc = 0.40878\n",
      "Batch 554: loss = 2.0910, acc = 0.37669\n",
      "Batch 555: loss = 1.9873, acc = 0.37162\n",
      "Batch 556: loss = 1.9062, acc = 0.47128\n",
      "Batch 557: loss = 1.7598, acc = 0.46284\n",
      "Batch 558: loss = 1.9452, acc = 0.44426\n",
      "Batch 559: loss = 1.7219, acc = 0.49493\n",
      "Batch 560: loss = 1.9808, acc = 0.45608\n",
      "Batch 561: loss = 1.8607, acc = 0.44088\n",
      "Batch 562: loss = 1.8503, acc = 0.44595\n",
      "Batch 563: loss = 1.8805, acc = 0.42399\n",
      "Batch 564: loss = 1.8335, acc = 0.44764\n",
      "Batch 565: loss = 1.9495, acc = 0.43750\n",
      "Batch 566: loss = 1.9983, acc = 0.42736\n",
      "Batch 567: loss = 2.1147, acc = 0.40203\n",
      "Batch 568: loss = 1.8258, acc = 0.46959\n",
      "Batch 569: loss = 1.9074, acc = 0.43581\n",
      "Batch 570: loss = 1.8397, acc = 0.47635\n",
      "Batch 571: loss = 2.0299, acc = 0.39189\n",
      "Batch 572: loss = 1.9193, acc = 0.42736\n",
      "Batch 573: loss = 2.0361, acc = 0.40541\n",
      "Batch 574: loss = 1.8288, acc = 0.46791\n",
      "Batch 575: loss = 1.8867, acc = 0.44257\n",
      "Batch 576: loss = 2.0167, acc = 0.39865\n",
      "Batch 577: loss = 1.8842, acc = 0.46284\n",
      "Batch 578: loss = 1.9700, acc = 0.42230\n",
      "Batch 579: loss = 1.8837, acc = 0.45608\n",
      "Batch 580: loss = 2.0179, acc = 0.43412\n",
      "Batch 581: loss = 1.7999, acc = 0.45101\n",
      "Batch 582: loss = 1.9887, acc = 0.39358\n",
      "Batch 583: loss = 1.7821, acc = 0.47466\n",
      "Batch 584: loss = 1.9349, acc = 0.43581\n",
      "Batch 585: loss = 1.9949, acc = 0.43074\n",
      "Batch 586: loss = 1.8006, acc = 0.50676\n",
      "Batch 587: loss = 1.9038, acc = 0.43581\n",
      "Batch 588: loss = 1.7354, acc = 0.47804\n",
      "Batch 589: loss = 1.7930, acc = 0.47466\n",
      "Batch 590: loss = 2.0275, acc = 0.40709\n",
      "Batch 591: loss = 1.8501, acc = 0.46284\n",
      "Batch 592: loss = 1.9529, acc = 0.41892\n",
      "Batch 593: loss = 1.9276, acc = 0.42736\n",
      "Batch 594: loss = 1.7515, acc = 0.50338\n",
      "Batch 595: loss = 1.9498, acc = 0.41554\n",
      "Batch 596: loss = 1.9298, acc = 0.41892\n",
      "Batch 597: loss = 1.8426, acc = 0.46791\n",
      "Batch 598: loss = 1.8861, acc = 0.44426\n",
      "Batch 599: loss = 1.8578, acc = 0.44595\n",
      "Batch 600: loss = 1.9287, acc = 0.45101\n",
      "Batch 601: loss = 1.8696, acc = 0.43243\n",
      "Batch 602: loss = 1.8863, acc = 0.45101\n",
      "Batch 603: loss = 1.8634, acc = 0.44595\n",
      "Batch 604: loss = 1.8745, acc = 0.44595\n",
      "Batch 605: loss = 1.8326, acc = 0.48142\n",
      "Batch 606: loss = 2.0439, acc = 0.41723\n",
      "Batch 607: loss = 1.9154, acc = 0.45439\n",
      "Batch 608: loss = 1.8239, acc = 0.46791\n",
      "Batch 609: loss = 2.0426, acc = 0.41385\n",
      "Batch 610: loss = 1.8839, acc = 0.44764\n",
      "Batch 611: loss = 1.8671, acc = 0.43581\n",
      "Batch 612: loss = 1.8404, acc = 0.45946\n",
      "Batch 613: loss = 1.8950, acc = 0.44932\n",
      "Batch 614: loss = 1.8552, acc = 0.43750\n",
      "Batch 615: loss = 1.7947, acc = 0.46284\n",
      "Batch 616: loss = 1.9815, acc = 0.43243\n",
      "Batch 617: loss = 1.9566, acc = 0.44088\n",
      "Batch 618: loss = 1.8584, acc = 0.44595\n",
      "Batch 619: loss = 1.8927, acc = 0.43750\n",
      "Batch 620: loss = 1.7946, acc = 0.47635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 621: loss = 1.8136, acc = 0.46959\n",
      "Batch 622: loss = 1.9125, acc = 0.43412\n",
      "Batch 623: loss = 1.8520, acc = 0.46622\n",
      "Batch 624: loss = 1.9391, acc = 0.42399\n",
      "Batch 625: loss = 1.8787, acc = 0.45270\n",
      "Batch 626: loss = 1.7699, acc = 0.47804\n",
      "Batch 627: loss = 1.9296, acc = 0.42230\n",
      "Batch 628: loss = 1.8095, acc = 0.48142\n",
      "Batch 629: loss = 1.8957, acc = 0.44088\n",
      "Batch 630: loss = 1.8999, acc = 0.42568\n",
      "Batch 631: loss = 1.8887, acc = 0.43750\n",
      "Batch 632: loss = 1.7784, acc = 0.48142\n",
      "Batch 633: loss = 1.8471, acc = 0.45777\n",
      "Batch 634: loss = 2.0084, acc = 0.42061\n",
      "Batch 635: loss = 1.8511, acc = 0.44257\n",
      "Batch 636: loss = 2.0269, acc = 0.42736\n",
      "Batch 637: loss = 1.8487, acc = 0.45946\n",
      "Batch 638: loss = 1.7424, acc = 0.49493\n",
      "Batch 639: loss = 1.7903, acc = 0.46284\n",
      "Batch 640: loss = 1.7628, acc = 0.46791\n",
      "Batch 641: loss = 1.8078, acc = 0.51182\n",
      "Batch 642: loss = 1.8436, acc = 0.46959\n",
      "Batch 643: loss = 1.8496, acc = 0.45101\n",
      "Batch 644: loss = 1.8660, acc = 0.47466\n",
      "Batch 645: loss = 1.6513, acc = 0.51858\n",
      "Batch 646: loss = 1.7664, acc = 0.46284\n",
      "Batch 647: loss = 1.8549, acc = 0.45101\n",
      "Batch 648: loss = 1.8710, acc = 0.46284\n",
      "Batch 649: loss = 1.7589, acc = 0.51014\n",
      "Batch 650: loss = 1.8536, acc = 0.43581\n",
      "Batch 651: loss = 1.7620, acc = 0.51858\n",
      "Batch 652: loss = 1.8268, acc = 0.46959\n",
      "Batch 653: loss = 1.8273, acc = 0.48142\n",
      "Batch 654: loss = 1.7296, acc = 0.51182\n",
      "Batch 655: loss = 1.7564, acc = 0.50338\n",
      "Batch 656: loss = 1.6770, acc = 0.50507\n",
      "Batch 657: loss = 1.7077, acc = 0.49662\n",
      "Batch 658: loss = 1.9664, acc = 0.44088\n",
      "Batch 659: loss = 1.7532, acc = 0.47635\n",
      "Batch 660: loss = 2.0025, acc = 0.42399\n",
      "Batch 661: loss = 1.8399, acc = 0.48480\n",
      "Batch 662: loss = 1.8631, acc = 0.48142\n",
      "Batch 663: loss = 1.7662, acc = 0.49662\n",
      "Batch 664: loss = 1.8956, acc = 0.44932\n",
      "Batch 665: loss = 1.8500, acc = 0.44764\n",
      "Batch 666: loss = 1.8297, acc = 0.46115\n",
      "Batch 667: loss = 1.7992, acc = 0.44764\n",
      "Batch 668: loss = 1.8419, acc = 0.45777\n",
      "Batch 669: loss = 1.7685, acc = 0.45777\n",
      "Batch 670: loss = 1.7930, acc = 0.47804\n",
      "Batch 671: loss = 1.7226, acc = 0.49493\n",
      "Batch 672: loss = 1.8322, acc = 0.45101\n",
      "Batch 673: loss = 1.8088, acc = 0.45608\n",
      "Batch 674: loss = 1.6151, acc = 0.51689\n",
      "Batch 675: loss = 1.7334, acc = 0.49155\n",
      "Batch 676: loss = 1.7355, acc = 0.49155\n",
      "Batch 677: loss = 2.0364, acc = 0.42568\n",
      "Batch 678: loss = 1.8535, acc = 0.45439\n",
      "Batch 679: loss = 1.8260, acc = 0.46284\n",
      "Batch 680: loss = 1.8547, acc = 0.46284\n",
      "Batch 681: loss = 1.9559, acc = 0.41216\n",
      "Batch 682: loss = 1.7017, acc = 0.48311\n",
      "Batch 683: loss = 1.8454, acc = 0.45439\n",
      "Batch 684: loss = 1.6940, acc = 0.49324\n",
      "Batch 685: loss = 1.8047, acc = 0.46791\n",
      "Batch 686: loss = 1.8634, acc = 0.45777\n",
      "Batch 687: loss = 1.8280, acc = 0.45439\n",
      "Batch 688: loss = 1.7453, acc = 0.47297\n",
      "Batch 689: loss = 1.7845, acc = 0.48818\n",
      "Batch 690: loss = 1.6725, acc = 0.50338\n",
      "Batch 691: loss = 1.9278, acc = 0.44932\n",
      "Batch 692: loss = 1.7558, acc = 0.47804\n",
      "Batch 693: loss = 1.8514, acc = 0.47297\n",
      "Batch 694: loss = 1.8293, acc = 0.44595\n",
      "Batch 695: loss = 1.8741, acc = 0.45439\n",
      "Batch 696: loss = 1.8390, acc = 0.47635\n",
      "Batch 697: loss = 1.8639, acc = 0.43919\n",
      "Batch 698: loss = 1.9209, acc = 0.43750\n",
      "Batch 699: loss = 1.9845, acc = 0.41554\n",
      "Batch 700: loss = 1.6917, acc = 0.50845\n",
      "Batch 701: loss = 1.8293, acc = 0.46453\n",
      "Batch 702: loss = 1.8081, acc = 0.44764\n",
      "Batch 703: loss = 1.9905, acc = 0.39358\n",
      "Batch 704: loss = 1.9883, acc = 0.41216\n",
      "Batch 705: loss = 1.9630, acc = 0.45608\n",
      "Batch 706: loss = 1.9542, acc = 0.45439\n",
      "Batch 707: loss = 2.1178, acc = 0.42399\n",
      "Batch 708: loss = 1.9137, acc = 0.43412\n",
      "Batch 709: loss = 1.9774, acc = 0.40878\n",
      "Batch 710: loss = 1.7624, acc = 0.46115\n",
      "Batch 711: loss = 1.8229, acc = 0.47297\n",
      "Batch 712: loss = 1.8889, acc = 0.44595\n",
      "Batch 713: loss = 1.7541, acc = 0.48480\n",
      "Batch 714: loss = 1.7824, acc = 0.45777\n",
      "Batch 715: loss = 1.6985, acc = 0.50845\n",
      "Batch 716: loss = 1.8962, acc = 0.46791\n",
      "Batch 717: loss = 1.7797, acc = 0.49831\n",
      "Batch 718: loss = 1.7807, acc = 0.49324\n",
      "Batch 719: loss = 1.8574, acc = 0.46115\n",
      "Batch 720: loss = 1.8894, acc = 0.42568\n",
      "Batch 721: loss = 1.7401, acc = 0.44764\n",
      "Batch 722: loss = 1.7950, acc = 0.47804\n",
      "Batch 723: loss = 1.7972, acc = 0.47973\n",
      "Batch 724: loss = 1.8133, acc = 0.48142\n",
      "Batch 725: loss = 1.9503, acc = 0.43581\n",
      "Batch 726: loss = 1.9192, acc = 0.41892\n",
      "Batch 727: loss = 1.7779, acc = 0.45777\n",
      "Batch 728: loss = 1.8435, acc = 0.43581\n",
      "Batch 729: loss = 1.7682, acc = 0.48480\n",
      "Batch 730: loss = 1.8257, acc = 0.45101\n",
      "Batch 731: loss = 1.7761, acc = 0.48818\n",
      "Batch 732: loss = 1.8165, acc = 0.46115\n",
      "Batch 733: loss = 1.7672, acc = 0.49324\n",
      "Batch 734: loss = 1.9697, acc = 0.43581\n",
      "Batch 735: loss = 1.7599, acc = 0.46622\n",
      "Batch 736: loss = 1.8232, acc = 0.46791\n",
      "Batch 737: loss = 1.7378, acc = 0.48142\n",
      "Batch 738: loss = 1.7578, acc = 0.49662\n",
      "Batch 739: loss = 1.7130, acc = 0.50507\n",
      "Batch 740: loss = 1.6745, acc = 0.50845\n",
      "Batch 741: loss = 1.5706, acc = 0.55236\n",
      "Batch 742: loss = 1.7505, acc = 0.47635\n",
      "Batch 743: loss = 1.6191, acc = 0.51689\n",
      "Batch 744: loss = 1.6331, acc = 0.51520\n",
      "Batch 745: loss = 1.7270, acc = 0.48649\n",
      "Batch 746: loss = 1.7505, acc = 0.46284\n",
      "Batch 747: loss = 1.7503, acc = 0.46622\n",
      "Batch 748: loss = 1.7166, acc = 0.49662\n",
      "Batch 749: loss = 1.9292, acc = 0.45946\n",
      "Batch 750: loss = 1.7172, acc = 0.50000\n",
      "Batch 751: loss = 1.8776, acc = 0.45101\n",
      "Batch 752: loss = 2.0003, acc = 0.41216\n",
      "Batch 753: loss = 1.8869, acc = 0.46284\n",
      "Batch 754: loss = 1.8098, acc = 0.49831\n",
      "Batch 755: loss = 1.5474, acc = 0.52534\n",
      "Batch 756: loss = 1.7571, acc = 0.46284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-79f9520553b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list_of_ints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch {}: loss = {:.4f}, acc = {:.5f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1066\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (X, Y) in enumerate(read_batches(data_list_of_ints, vocab_size)):\n",
    "    loss, acc = model.train_on_batch(X, Y)\n",
    "    print('Batch {}: loss = {:.4f}, acc = {:.5f}'.format(i + 1, loss, acc))\n",
    "    \n",
    "    if i == 2000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "X,Y = next(read_batches(data_list_of_ints, vocab_size))\n",
    "\n",
    "def get_layer_i_output_on_X(i, X):\n",
    "    \n",
    "    X = X.reshape(-1, SEQ_LENGTH)\n",
    "    get_i_layer_output = K.function([model.layers[0].input],\n",
    "                                    [model.layers[i].output])\n",
    "    layer_output = get_i_layer_output([X])[0]\n",
    "    \n",
    "    return layer_output\n",
    "\n",
    "def get_layer_j_parameters(j):\n",
    "    layer = model.layers[j]\n",
    "    n_units = layer.units\n",
    "    \n",
    "    kernel_i = layer.get_weights()[0][:,:n_units]\n",
    "    kernel_f = layer.get_weights()[0][:,n_units:2*n_units]\n",
    "    kernel_c = layer.get_weights()[0][:,2*n_units:3*n_units]\n",
    "    kernel_o = layer.get_weights()[0][:,3*n_units:]\n",
    "    \n",
    "    recur_i = layer.get_weights()[1][:,:n_units]\n",
    "    recur_f = layer.get_weights()[1][:,n_units:2*n_units]\n",
    "    recur_c = layer.get_weights()[1][:,2*n_units:3*n_units]\n",
    "    recur_o = layer.get_weights()[1][:,3*n_units:]\n",
    "    \n",
    "    bias_i = layer.get_weights()[2][:n_units]\n",
    "    bias_f = layer.get_weights()[2][n_units:2*n_units]\n",
    "    bias_c = layer.get_weights()[2][2*n_units:3*n_units]\n",
    "    bias_o = layer.get_weights()[2][3*n_units:]\n",
    "    \n",
    "    dict_with_params = {'kernel_i':kernel_i,\n",
    "                        'kernel_f':kernel_f,\n",
    "                        'kernel_c':kernel_c,\n",
    "                        'kernel_o':kernel_o,\n",
    "                        'recur_i':recur_i,\n",
    "                        'recur_f':recur_f,\n",
    "                        'recur_c':recur_c,\n",
    "                        'recur_o':recur_o,\n",
    "                        'bias_i':bias_i,\n",
    "                        'bias_f':bias_f,\n",
    "                        'bias_c':bias_c,\n",
    "                        'bias_o':bias_o,}\n",
    "    \n",
    "    return dict_with_params\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    return (x < -2.5).astype(int) * 0 + (x > 2.5).astype(int) * 1 + ((-2.5<=x) & (x<=2.5)).astype(int) * (0.2 * x + 0.5)\n",
    "\n",
    "def evaluate_layer_j_on_input(j, x, h, c):\n",
    "    d = get_layer_j_parameters(j)\n",
    "    \n",
    "    x = x.reshape(1,-1); h = h.reshape(1,-1); c = c.reshape(1,-1) \n",
    "    \n",
    "    i = hard_sigmoid(x @ d['kernel_i'] + h @ d['recur_i'] + d['bias_i'])\n",
    "    f = hard_sigmoid(x @ d['kernel_f'] + h @ d['recur_f'] + d['bias_f'])\n",
    "    c = f * c + i * np.tanh(x @ d['kernel_c'] + h @ d['recur_c'] + d['bias_c'])\n",
    "    o = hard_sigmoid(x @ d['kernel_o'] + h @ d['recur_o'] + d['bias_o'])\n",
    "    \n",
    "    h = o * np.tanh(c)\n",
    "    return h, c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = get_layer_i_output_on_X(2, X)\n",
    "inp = inp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36997015, -0.20243904,  0.06580108, -0.08376692, -0.49079166,\n",
       "         0.48357672,  0.42694387, -0.87016739,  0.4308302 , -0.08088644,\n",
       "         0.91210242, -0.17258206,  0.40084778, -0.41717416, -0.51342262,\n",
       "         0.44991216,  0.02379311, -0.4938219 , -0.87104302,  0.76510008,\n",
       "        -0.60268491, -0.152367  , -0.02231249,  0.53911072,  0.19190407,\n",
       "         0.72176333, -0.13597204,  0.        ,  0.19019175, -0.74812226,\n",
       "        -0.01043332,  0.        , -0.03869087,  0.70331416,  0.6289672 ,\n",
       "         0.00245176, -0.47211593,  0.23353648,  0.77533029, -0.75906905,\n",
       "        -0.7471119 , -0.08046857, -0.47160222,  0.67509451, -0.61650389,\n",
       "        -0.82328658, -0.62277423,  0.67052023, -0.39676215, -0.10206082,\n",
       "        -0.59735924,  0.27712175, -0.71738176, -0.27167856,  0.78664425,\n",
       "         0.59797541, -0.34370182, -0.99572807,  0.02462915,  0.43336879,\n",
       "         0.00970127,  0.24647308, -0.89373355, -0.67407787,  0.4244087 ,\n",
       "         0.17682114, -0.73011373, -0.92515331,  0.18333938, -0.74488849,\n",
       "        -0.92046635, -0.35229212,  0.38629935, -0.17606909, -0.60721924,\n",
       "        -0.74738502, -0.01735626,  0.73478297, -0.        ,  0.50743362,\n",
       "        -0.        , -0.99451453, -0.47084937, -0.30230904, -0.60176407,\n",
       "        -0.17561223, -0.37253861,  0.9228915 ,  0.00643774, -0.35201271,\n",
       "         0.36370522, -0.17225905,  0.55450601,  0.67412065, -0.20195515,\n",
       "         0.61443895, -0.20801041, -0.17850191, -0.34559829,  0.        ,\n",
       "        -0.12180072,  0.51111944,  0.72721819,  0.        , -0.08177112,\n",
       "         0.        , -0.62537359,  0.        , -0.6643734 , -0.69037963,\n",
       "        -0.58040894, -0.9994091 ,  0.03012557, -0.47903116,  0.53278575,\n",
       "         0.18290021,  0.86414926,  0.        ,  0.49729099, -0.62540515,\n",
       "        -0.58347589, -0.73728056,  0.57202429, -0.99995271, -0.00434724,\n",
       "        -0.84662328,  0.00208303, -0.20411598,  0.77805584, -0.66303416,\n",
       "         0.62057936, -0.33471863,  0.6845413 ,  0.05321665,  0.14042399,\n",
       "        -0.        , -0.85603619, -0.10993093,  0.18732115,  0.55053325,\n",
       "         0.59551661, -0.63428318, -0.98404968, -0.09309251, -0.75561222,\n",
       "        -0.62459269, -0.20289368, -0.60340686,  0.03122134, -0.39085582,\n",
       "        -0.57732376,  0.21672296, -0.        ,  0.61890249, -0.75372084,\n",
       "         0.69446194, -0.67007459,  0.12200616,  0.57294589, -0.48003566,\n",
       "         0.16833092,  0.        ,  0.        ,  0.66652618,  0.06268156,\n",
       "        -0.9462093 ,  0.57203104, -0.84110261,  0.8613652 , -0.50276962,\n",
       "         0.82570803, -0.47222974,  0.62069437,  0.87252967,  0.3880439 ,\n",
       "         0.7086006 , -0.7477166 , -0.5783809 ,  0.9534806 ,  0.71818609,\n",
       "        -0.61388097,  0.75885615, -0.01702666,  0.        , -0.19443043,\n",
       "        -0.23927758, -0.0886364 , -0.71475659,  0.48628132,  0.44709406,\n",
       "         0.57919132, -0.01066908, -0.40506996,  0.63761382, -0.75141388,\n",
       "         0.62948685,  0.85538621,  0.00693532, -0.75461222,  0.47641098,\n",
       "        -0.4463786 ,  0.44370481,  0.41145274, -0.53831457,  0.73509314,\n",
       "        -0.79004559,  0.32512594, -0.65851936, -0.58195305,  0.73876014,\n",
       "         0.69289614, -0.42457229,  0.86274343, -0.04948674,  0.45489439,\n",
       "        -0.        , -0.20863807,  0.03393449, -0.5985584 ,  0.80787783,\n",
       "         0.36968181,  0.78613571, -0.73392414, -0.53069045, -0.42026157,\n",
       "         0.93092411,  0.4449196 ,  0.00570439, -0.70237364,  0.14160769,\n",
       "        -0.21213713,  0.95552435,  0.03083209, -0.49863513, -0.31076916,\n",
       "        -0.57178056,  0.03932923, -0.16816955,  0.29480777,  0.45577073,\n",
       "         0.27987246,  0.45362154, -0.24644477, -0.5706073 , -0.        ,\n",
       "        -0.5280216 , -0.75983734,  0.78926333,  0.27713671,  0.94912518,\n",
       "         0.21208293, -0.18017369, -0.05977267,  0.6737992 ,  0.13655855,\n",
       "        -0.06687833]])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, c = np.zeros(256), np.zeros(256)\n",
    "\n",
    "for i in range(37):\n",
    "    h, c = evaluate_layer_j_on_input(3, \n",
    "                                      inp[i],\n",
    "                                      h,\n",
    "                                      c)\n",
    "    if i == 10: break\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36934754, -0.20502695,  0.07099158, -0.07985611, -0.49090534,\n",
       "        0.4773793 ,  0.4237625 , -0.8673782 ,  0.43301553, -0.08046973,\n",
       "        0.91521806, -0.16861847,  0.5445328 , -0.4210522 , -0.5043266 ,\n",
       "        0.45119017,  0.02018598, -0.500369  , -0.86776054,  0.7624329 ,\n",
       "       -0.59585583, -0.15962595, -0.0213448 ,  0.589288  ,  0.18945381,\n",
       "        0.71223366, -0.10085215,  0.        ,  0.18663205, -0.747557  ,\n",
       "       -0.01136942,  0.        , -0.04426956,  0.70020896,  0.65854555,\n",
       "        0.00374589, -0.49357435,  0.23091209,  0.76838475, -0.7691674 ,\n",
       "       -0.7381811 , -0.2151977 , -0.46745336,  0.67120683, -0.62464386,\n",
       "       -0.819101  , -0.6256754 ,  0.6852928 , -0.39654216, -0.09991623,\n",
       "       -0.59631824,  0.35408163, -0.71468186, -0.2691934 ,  0.7848156 ,\n",
       "        0.60598415, -0.34383225, -1.        ,  0.02580756,  0.432583  ,\n",
       "        0.02433919,  0.25338635, -0.89056635, -0.6769468 ,  0.4226778 ,\n",
       "        0.18839926, -0.7309489 , -0.95304793,  0.191396  , -0.7457945 ,\n",
       "       -0.92041445, -0.33433697,  0.38310367, -0.15420717, -0.6104244 ,\n",
       "       -0.7750111 , -0.01772521,  0.7352674 , -0.        ,  0.51187533,\n",
       "       -0.        , -0.9950279 , -0.4684053 , -0.2981775 , -0.6018295 ,\n",
       "       -0.17922181, -0.3887534 ,  0.9230702 ,  0.00400755, -0.34801885,\n",
       "        0.36469957, -0.17740518,  0.55580765,  0.6716156 , -0.19961512,\n",
       "        0.6144136 , -0.21401937, -0.1917605 , -0.34382454,  0.        ,\n",
       "       -0.11386541,  0.5118477 ,  0.72667116,  0.        , -0.08469748,\n",
       "        0.        , -0.6289564 ,  0.        , -0.66343325, -0.69282365,\n",
       "       -0.5777621 , -0.99999654,  0.02987364, -0.4783379 ,  0.5323767 ,\n",
       "        0.18096514,  0.8604659 ,  0.        ,  0.52142537, -0.6357554 ,\n",
       "       -0.587742  , -0.80919015,  0.5810432 , -1.        , -0.00666262,\n",
       "       -0.84816855,  0.        , -0.21070169,  0.7610374 , -0.66551304,\n",
       "        0.6224483 , -0.32381266,  0.6952333 ,  0.05116713,  0.13818187,\n",
       "       -0.        , -0.86059356, -0.33590907,  0.17037278,  0.58341575,\n",
       "        0.5943053 , -0.6346394 , -0.9841875 , -0.09103052, -0.75587296,\n",
       "       -0.63042283, -0.19767246, -0.6012064 ,  0.03102779, -0.40364495,\n",
       "       -0.5768179 ,  0.10070453, -0.        ,  0.7363266 , -0.75637937,\n",
       "        0.6894571 , -0.67317873,  0.11716461,  0.5804043 , -0.4736943 ,\n",
       "        0.19645604,  0.        ,  0.        ,  0.6638514 ,  0.06032578,\n",
       "       -0.94797105,  0.6018124 , -0.8394847 ,  0.9099321 , -0.50614136,\n",
       "        0.82530487, -0.48954836,  0.62249935,  0.86629266,  0.38562587,\n",
       "        0.7013495 , -0.7473631 , -0.57546943,  0.9525973 ,  0.7106945 ,\n",
       "       -0.6077955 ,  0.75885177, -0.01923708,  0.        , -0.19406964,\n",
       "       -0.24224943, -0.09504034, -0.76423633,  0.4876566 ,  0.44841343,\n",
       "        0.57619435, -0.0088763 , -0.3998911 ,  0.6483533 , -0.7512297 ,\n",
       "        0.6195047 ,  0.86479163,  0.00882352, -0.7547566 ,  0.47633094,\n",
       "       -0.4446745 ,  0.44281596,  0.41140687, -0.54065734,  0.73569596,\n",
       "       -0.79552495,  0.32109296, -0.6504102 , -0.5829336 ,  0.734737  ,\n",
       "        0.69004554, -0.4281677 ,  0.85980594, -0.04923311,  0.45208642,\n",
       "       -0.        , -0.20440246,  0.03629464, -0.6049314 ,  0.79954237,\n",
       "        0.37265277,  0.80203676, -0.7315422 , -0.52830446, -0.42058808,\n",
       "        0.9318815 ,  0.4440801 ,  0.0063764 , -0.8839464 ,  0.14508142,\n",
       "       -0.22244272,  0.99497056,  0.04660984, -0.4907847 , -0.32410696,\n",
       "       -0.56503445, -0.4764371 , -0.16642737,  0.29197434,  0.4611846 ,\n",
       "        0.27600887,  0.45224363, -0.24488537, -0.56776404, -0.        ,\n",
       "       -0.52680093, -0.7829546 ,  0.79216385,  0.21740408,  0.9469239 ,\n",
       "        0.20530331, -0.2472082 , -0.05904821,  0.6717506 ,  0.13669698,\n",
       "       -0.06854339], dtype=float32)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = get_layer_i_output_on_X(3, X)[0][10]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011895180755745461"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(h-r).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.93136036, -1.        , -0.17444812, -0.68670225,  0.4133347 ,\n",
       "       -0.5250608 , -0.79048383, -0.72331417,  0.        , -0.14976543,\n",
       "        0.        , -0.6946399 ,  0.91270244,  0.75910044,  0.22572684,\n",
       "        0.07792626,  0.        , -0.08412182, -0.20709842, -0.3203006 ,\n",
       "        0.65081555,  0.85162616, -0.9411663 ,  0.5341775 ,  0.42733675,\n",
       "       -0.        ,  0.03931336,  0.76374185,  0.16513526, -0.08022057,\n",
       "       -0.8024534 ,  0.46092355, -1.        , -0.37287325,  0.58876956,\n",
       "        0.06760677, -0.27462685,  0.41723663,  0.01741885,  0.77375   ,\n",
       "        0.        , -0.4277774 , -0.5852314 ,  0.        , -0.8089063 ,\n",
       "       -0.03152403,  0.        ,  0.27586657, -0.33503497,  0.8570576 ,\n",
       "        0.77225584,  0.05322284,  0.727936  ,  0.        , -0.        ,\n",
       "       -0.75565785, -0.        , -1.        , -0.70560694,  0.        ,\n",
       "        0.58829767,  0.72832584, -0.        ,  0.03522956, -0.74437356,\n",
       "       -0.6367787 , -0.59861875, -0.19066182, -0.63795483, -0.31388286,\n",
       "       -0.4277712 , -0.        ,  0.95391184,  0.10201892,  0.8206357 ,\n",
       "       -0.5295227 , -0.6057494 , -0.14393504, -0.99999243, -0.76129353,\n",
       "        0.2009692 , -0.07699083, -0.12122629, -0.30100816,  0.92729306,\n",
       "        0.72118497,  0.2911945 ,  0.21963672,  1.        , -0.        ,\n",
       "       -0.04027814, -0.7340306 , -0.3291013 , -0.99547666,  0.16005537,\n",
       "        0.        ,  0.14292717, -0.7174926 , -0.        , -0.4790202 ,\n",
       "       -0.13721934,  0.        ,  0.        ,  1.        ,  0.75941074,\n",
       "        1.        , -0.4344849 , -0.95771265,  0.9687056 ,  0.4759191 ,\n",
       "       -0.        , -0.        , -0.05479313, -0.9704098 ,  0.8812849 ,\n",
       "       -0.6199775 ,  0.5858168 , -0.6228636 , -0.6844209 ,  0.34794748,\n",
       "       -0.84177554, -0.19830999,  0.        , -0.97359616, -0.73928976,\n",
       "       -0.        ,  0.99277943,  0.6020156 , -0.06316724,  0.46236166,\n",
       "        0.37231997, -0.        ,  0.11791083,  0.3862838 ,  0.9999336 ,\n",
       "        0.9912468 ,  0.6030687 , -0.9991994 , -0.12348576,  0.        ,\n",
       "       -0.35778147,  0.7435861 , -0.05287381,  0.27118033,  0.0150452 ,\n",
       "       -0.15220027,  0.22935575,  0.        , -0.51992774, -0.12015279,\n",
       "        0.4460924 , -0.        ,  0.7603586 ,  0.        ,  0.9574622 ,\n",
       "       -0.        , -0.29383928,  0.16278249,  0.01349447,  0.        ,\n",
       "        1.        ,  0.9911069 , -0.8567209 ,  0.351461  , -0.45138803,\n",
       "       -0.32540622,  0.04512262,  0.        ,  0.89448154, -0.715654  ,\n",
       "       -0.1902003 , -0.22925301,  0.        ,  0.12070528,  0.9868438 ,\n",
       "        0.48065594,  0.4957718 , -0.85413134,  0.68617415,  0.        ,\n",
       "       -0.        , -0.34834242,  0.6752434 ,  0.95785505,  0.757494  ,\n",
       "       -1.        ,  0.00615906, -0.        , -0.16002117,  0.3682771 ,\n",
       "        0.43868664,  0.84301734, -0.5937665 ,  0.15668908,  0.        ,\n",
       "       -0.03380422,  0.35458893,  0.        , -0.        , -0.20662957,\n",
       "       -0.        ,  0.48356143,  0.4454836 , -0.44946215,  0.        ,\n",
       "       -0.3852224 ,  0.7232995 ,  0.        ,  0.        , -0.00936854,\n",
       "        0.        , -0.24095653,  0.06667025,  0.9490436 , -0.        ,\n",
       "       -0.43914524, -0.90829676, -0.82202774,  0.02527277, -0.1484626 ,\n",
       "        0.65333295,  0.68272984, -0.        ,  0.0045276 , -0.43185186,\n",
       "       -0.8107656 ,  0.22730123,  0.29698372, -0.7210494 , -0.94782937,\n",
       "       -0.80147773,  0.        ,  0.629557  , -0.941399  , -0.        ,\n",
       "       -0.        , -0.41715473,  0.3930541 , -0.93792164,  0.24618687,\n",
       "        0.50803334,  0.21084902,  0.6316229 ,  0.42276698,  0.        ,\n",
       "       -0.        , -0.88684577, -0.62307715, -0.        , -0.03539616,\n",
       "       -0.99176705, -1.        , -0.60951567, -0.        , -0.81567895,\n",
       "       -0.8784998 ], dtype=float32)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = K.function([model.layers[2].output], [model.layers[3].output])\n",
    "f([get_layer_i_output_on_X(2, X)])[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999998942"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(r-h).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = model.layers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'lstm_24/kernel:0' shape=(256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_24/recurrent_kernel:0' shape=(256, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_24/bias:0' shape=(1024,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layer_output[0]\n",
    "mat = q.get_weights()[1][:, :256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = layer_output[0] @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 256), (37, 256), (37, 256))"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape, inp.shape, res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 256)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.cell.unit_forget_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
